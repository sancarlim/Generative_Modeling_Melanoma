{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "siim-isic-pytorch-lightning-starter-seresnext50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandracl72/Generative_Modeling_Melanoma/blob/main/siim_isic_pytorch_lightning_starter_seresnext50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO-mwI_QZzqy"
      },
      "source": [
        "Note: The io with original images is awful. Therefore I created a dataset https://www.kaggle.com/arroqc/siic-isic-224x224-images of images preprocessed to 224x224 size and saved in png (lossless).\n",
        "If you train on original jpeg large imagesm I have put the right lines of code in comments with the added comments : # Use this when training with original images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uExA-fCPZzq0"
      },
      "source": [
        "# Pytorch Lightning Starter - SSIM Melanoma \n",
        "\n",
        "Why use Pytorch-Lightning ?\n",
        "\n",
        "Pytorch lighntning is designed to help you easily follow a pytorch based training loop and ease modifications that you may want. Want to use a new scheduler ? Then simply modify the configure_optimizer method ! The beauty of it is that it automates all the boring stuff that clogs a pure pytorch code. All these loops, .zero_grad(), .eval(), torch.save etc. are gone and handled by the framework. You just have to focus on the ML part of it. The best things for researchers is that it comes with automated logs through tensorboard to compare your many experiments and easy switches between GPU, DataParallel, TPU mixed precision etc.  I suggest reproducing the code of this kernel in a local environment and use tensorboard there.\n",
        "\n",
        "You may ask why not simply use fastai. This is now a matter of preference. Fastai automates a lot of stuff with best practices like .fit_one_cycle. But on the other hand unless you have a lot of experience with it I find it rather opaque in what is happening behind the scenes. It's a framework designed to go with doing the fastai course so that you understand the options. If like me you learnt deep learning in a more academic environment in pure pytorch or pure tensorflow then you may find fastai hard to understand without listening to J. Howard courses. Similarly as soon as you want to do something a bit different it can become hard to understand how to change anything. On a personal note, I'll wait for the fastai v2 course before delving into it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iToKqA8-Zzq2"
      },
      "source": [
        "!pip install pytorch-lightning "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-5st9TMZzq3"
      },
      "source": [
        "\n",
        "## Loading data\n",
        "\n",
        "First let's open the csv. One thing we need to make sure when splitting data in a medical context is to split by patient ID rather than image ID. Otherwise you run the risk of having some data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "_8KRELqxZzq3"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import PIL.Image as Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch.utils.data as tdata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MlL400qdx1Z",
        "outputId": "10ce9cff-3d6f-4417-dc7d-daf4bf93a963"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/MyDrive/SAHLGRENSKA AI Competence Center/Colab Notebooks'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "cuUgpHIKed2V",
        "outputId": "82e4b326-8375-4690-c959-4bf3c373fc7e"
      },
      "source": [
        "\n",
        "from google.colab import files\n",
        "files.upload() #this will prompt you to update the json\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ecf6cc85-8e07-4e79-858f-54d0120b41f7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ecf6cc85-8e07-4e79-858f-54d0120b41f7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peXqEEaFcmbr"
      },
      "source": [
        "\n",
        "#Configuration environment\n",
        "import os\n",
        "\n",
        "!kaggle competitions download -c siim-isic-melanoma-classification -p /content/gdrive/MyDrive/SAHLGRENSKA\\ AI\\ Competence\\ Center/Colab\\ Notebooks/ISIC20\\ DATASET\n",
        "'''\n",
        "#List datasets and competitions\n",
        "!kaggle datasets list   \n",
        "!kaggle competitions list\n",
        "\n",
        "#Search for datasets and competitions by name\n",
        "!kaggle datasets list -s melanoma\n",
        "!kaggle competitions list -s siim-isic-melanoma-classification\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AaTlC7O1Zzq4"
      },
      "source": [
        "# Reproductibility\n",
        "SEED = 33\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "SlZRooiEZzq5"
      },
      "source": [
        "def dict_to_args(d):\n",
        "\n",
        "    args = argparse.Namespace()\n",
        "\n",
        "    def dict_to_args_recursive(args, d, prefix=''):\n",
        "        for k, v in d.items():\n",
        "            if type(v) == dict:\n",
        "                dict_to_args_recursive(args, v, prefix=k)\n",
        "            elif type(v) in [tuple, list]:\n",
        "                continue\n",
        "            else:\n",
        "                if prefix:\n",
        "                    args.__setattr__(prefix + '_' + k, v)\n",
        "                else:\n",
        "                    args.__setattr__(k, v)\n",
        "\n",
        "    dict_to_args_recursive(args, d)\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnpX_EQzcVCd",
        "outputId": "76493f69-f08d-4fe2-e4c1-feac1c229964"
      },
      "source": [
        "!kaggle datasets download -d arroqc/siic-isic-224x224-images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading siic-isic-224x224-images.zip to /content\n",
            "100% 3.27G/3.27G [00:51<00:00, 36.2MB/s]\n",
            "100% 3.27G/3.27G [00:51<00:00, 68.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ujxyPledrM"
      },
      "source": [
        "!kaggle competitions download -c siim-isic-melanoma-classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ufCp9namZzq6"
      },
      "source": [
        "train_df = pd.read_csv('train.csv.zip')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "#IMAGE_DIR = Path('/kaggle/input/siim-isic-melanoma-classification/jpeg')  # Use this when training with original images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K5cZiFVl-D0"
      },
      "source": [
        " !unzip siic-isic-224x224-images.zip\n",
        " IMAGE_DIR = Path('siic-isic-224x224-images/')\n",
        " Image.open(str(IMAGE_DIR / ('test/' + 'ISIC_0052060.png')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Ow7ZklWcZzq7",
        "outputId": "e944364b-4c42-49d6-8db6-c6fe581b2918"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>sex</th>\n",
              "      <th>age_approx</th>\n",
              "      <th>anatom_site_general_challenge</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>benign_malignant</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_2637011</td>\n",
              "      <td>IP_7279968</td>\n",
              "      <td>male</td>\n",
              "      <td>45.0</td>\n",
              "      <td>head/neck</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_0015719</td>\n",
              "      <td>IP_3075186</td>\n",
              "      <td>female</td>\n",
              "      <td>45.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_0052212</td>\n",
              "      <td>IP_2842074</td>\n",
              "      <td>female</td>\n",
              "      <td>50.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>nevus</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_0068279</td>\n",
              "      <td>IP_6890425</td>\n",
              "      <td>female</td>\n",
              "      <td>45.0</td>\n",
              "      <td>head/neck</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_0074268</td>\n",
              "      <td>IP_8723313</td>\n",
              "      <td>female</td>\n",
              "      <td>55.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     image_name  patient_id     sex  ...  diagnosis benign_malignant target\n",
              "0  ISIC_2637011  IP_7279968    male  ...    unknown           benign      0\n",
              "1  ISIC_0015719  IP_3075186  female  ...    unknown           benign      0\n",
              "2  ISIC_0052212  IP_2842074  female  ...      nevus           benign      0\n",
              "3  ISIC_0068279  IP_6890425  female  ...    unknown           benign      0\n",
              "4  ISIC_0074268  IP_8723313  female  ...    unknown           benign      0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qM3zntSZzq7",
        "outputId": "a3ea92e6-7520-40b6-c71c-a9b2486f922e"
      },
      "source": [
        "train_df.groupby(by=['patient_id'])['image_name'].count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "patient_id\n",
              "IP_0001230     6\n",
              "IP_0019713    13\n",
              "IP_0036322     5\n",
              "IP_0038436     3\n",
              "IP_0038545    28\n",
              "              ..\n",
              "IP_9989332    10\n",
              "IP_9992027    39\n",
              "IP_9995095    14\n",
              "IP_9996429     3\n",
              "IP_9997715    11\n",
              "Name: image_name, Length: 2056, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lTxW38qZzq8",
        "outputId": "d295198f-9474-484f-8361-82d8ff0714b4"
      },
      "source": [
        "train_df.groupby(by=['patient_id'])['target'].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "patient_id\n",
              "IP_0001230    0.000000\n",
              "IP_0019713    0.000000\n",
              "IP_0036322    0.000000\n",
              "IP_0038436    0.000000\n",
              "IP_0038545    0.035714\n",
              "                ...   \n",
              "IP_9989332    0.000000\n",
              "IP_9992027    0.000000\n",
              "IP_9995095    0.000000\n",
              "IP_9996429    0.000000\n",
              "IP_9997715    0.272727\n",
              "Name: target, Length: 2056, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2uRb67lZzq8"
      },
      "source": [
        "So you have patients that have multiple images. Also apparently the data is imbalanced. Let's verify:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "rB4aP4pvZzq9",
        "outputId": "2be9f7df-dfcd-412e-a314-202de164afd0"
      },
      "source": [
        "train_df.groupby(['target']).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>sex</th>\n",
              "      <th>age_approx</th>\n",
              "      <th>anatom_site_general_challenge</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>benign_malignant</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32542</td>\n",
              "      <td>32542</td>\n",
              "      <td>32477</td>\n",
              "      <td>32474</td>\n",
              "      <td>32024</td>\n",
              "      <td>32542</td>\n",
              "      <td>32542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>584</td>\n",
              "      <td>584</td>\n",
              "      <td>584</td>\n",
              "      <td>584</td>\n",
              "      <td>575</td>\n",
              "      <td>584</td>\n",
              "      <td>584</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        image_name  patient_id  ...  diagnosis  benign_malignant\n",
              "target                          ...                             \n",
              "0            32542       32542  ...      32542             32542\n",
              "1              584         584  ...        584               584\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF0wNs_LZzq9"
      },
      "source": [
        "so we have approx 60 times more negatives than positives. We need to make sure we split good/bad patients equally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sDLC-dmvZzq9"
      },
      "source": [
        "patient_means = train_df.groupby(['patient_id'])['target'].mean()\n",
        "patient_ids = train_df['patient_id'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xSxv89LzZzq-"
      },
      "source": [
        "# Now let's make our split\n",
        "# stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n",
        "train_idx, val_idx = train_test_split(np.arange(len(patient_ids)), stratify=(patient_means > 0), test_size=0.2)  # KFold + averaging should be much better considering how small the dataset is for malignant cases\n",
        "pid_train = patient_ids[train_idx]\n",
        "pid_val = patient_ids[val_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTJ_SyDCZzq-"
      },
      "source": [
        "Let's verify the split was correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "MAws6zQGZzq-",
        "outputId": "f9d41d77-b776-4237-b010-580188c23333"
      },
      "source": [
        "train_df[train_df['patient_id'].isin(pid_train)].groupby(['target']).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>sex</th>\n",
              "      <th>age_approx</th>\n",
              "      <th>anatom_site_general_challenge</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>benign_malignant</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25702</td>\n",
              "      <td>25702</td>\n",
              "      <td>25685</td>\n",
              "      <td>25682</td>\n",
              "      <td>25351</td>\n",
              "      <td>25702</td>\n",
              "      <td>25702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>444</td>\n",
              "      <td>444</td>\n",
              "      <td>444</td>\n",
              "      <td>444</td>\n",
              "      <td>438</td>\n",
              "      <td>444</td>\n",
              "      <td>444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        image_name  patient_id  ...  diagnosis  benign_malignant\n",
              "target                          ...                             \n",
              "0            25702       25702  ...      25702             25702\n",
              "1              444         444  ...        444               444\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "jOkVcumtZzq_",
        "outputId": "a150ffae-2cd3-4dba-e714-400a5d10cc2d"
      },
      "source": [
        "train_df[train_df['patient_id'].isin(pid_val)].groupby(['target']).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>sex</th>\n",
              "      <th>age_approx</th>\n",
              "      <th>anatom_site_general_challenge</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>benign_malignant</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6840</td>\n",
              "      <td>6840</td>\n",
              "      <td>6792</td>\n",
              "      <td>6792</td>\n",
              "      <td>6673</td>\n",
              "      <td>6840</td>\n",
              "      <td>6840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>140</td>\n",
              "      <td>140</td>\n",
              "      <td>140</td>\n",
              "      <td>140</td>\n",
              "      <td>137</td>\n",
              "      <td>140</td>\n",
              "      <td>140</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        image_name  patient_id  ...  diagnosis  benign_malignant\n",
              "target                          ...                             \n",
              "0             6840        6840  ...       6840              6840\n",
              "1              140         140  ...        140               140\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkrqJ8MkZzq_"
      },
      "source": [
        "## Pytorch Dataset\n",
        " A dataset should simply return all the information necessary for a sample by defining the getitem and len magic methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dIEeo33JZzq_"
      },
      "source": [
        "class SIIMDataset(tdata.Dataset):\n",
        "    \n",
        "    def __init__(self, df, transform, test=False):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.test = test\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        meta = self.df.iloc[idx]\n",
        "        #image_fn = meta['image_name'] + '.jpg'  # Use this when training with original images\n",
        "        image_fn = meta['image_name'] + '.png'\n",
        "        if self.test:\n",
        "            img = Image.open(str(IMAGE_DIR / ('test/' + image_fn)))\n",
        "        else:\n",
        "            img = Image.open(str(IMAGE_DIR / ('train/' + image_fn)))\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        if self.test:\n",
        "            return {'image': img}\n",
        "        else:\n",
        "            return {'image': img, 'target': meta['target']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB5gcV3GZzrA"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "I6dUf3lFZzrA"
      },
      "source": [
        "class AdaptiveConcatPool2d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.avg = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "        self.max = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_x = self.avg(x)\n",
        "        max_x = self.max(x)\n",
        "        return torch.cat([avg_x, max_x], dim=1)\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "    \n",
        "\n",
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_out=1, arch='resnet34'):\n",
        "        super().__init__()\n",
        "        if arch == 'resnet34':\n",
        "            remove_range = 2\n",
        "            m = models.resnet34(pretrained=True)\n",
        "        elif arch == 'seresnext50':\n",
        "            m = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnext50_32x4d_ssl')\n",
        "            remove_range = 2\n",
        "            \n",
        "        c_feature = list(m.children())[-1].in_features\n",
        "        self.base = nn.Sequential(*list(m.children())[:-remove_range])\n",
        "        self.head = nn.Sequential(AdaptiveConcatPool2d(),\n",
        "                                  Flatten(),\n",
        "                                  nn.Linear(c_feature * 2, c_out))\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.base(x)\n",
        "        logits = self.head(h).squeeze(1)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYd8QVrIZzrA"
      },
      "source": [
        "\n",
        "## Pytorch Lightning module definition\n",
        "\n",
        "In a normal pytorch code you probably would instantiate the model, dataloaders and make a nested for loop for epochs and batches. Pytorch lightning automates the engineering parts like the loops so that you focus on the ML part. To do that you create a pytorch lightning model and then define every ML step inside of it. To help you understand I have added comments under every method you need to implement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-Z0yYnsuZzrA"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        else:\n",
        "            return F_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HqedEdEkZzrB"
      },
      "source": [
        "class LightModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, df_train, df_test, pid_train, pid_val, hparams):\n",
        "        # This is where paths and options should be stored. I also store the\n",
        "        # train_idx, val_idx for cross validation since the dataset are defined \n",
        "        # in the module !\n",
        "        super().__init__()\n",
        "        self.pid_train = pid_train\n",
        "        self.pid_val = pid_val\n",
        "        self.df_train = df_train\n",
        "\n",
        "        self.model = Model(arch=hparams.arch)  # You will obviously want to make the model better :)\n",
        "\n",
        "        self.hparams = hparams\n",
        "        \n",
        "        # Defining datasets here instead of in prepare_data usually solves a lot of problems for me...\n",
        "        self.transform_train = transforms.Compose([#transforms.Resize((224, 224)),   # Use this when training with original images\n",
        "                                              transforms.RandomHorizontalFlip(0.5),\n",
        "                                              transforms.RandomVerticalFlip(0.5),\n",
        "                                              transforms.ToTensor(),\n",
        "                                              transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                   std=[0.229, 0.224, 0.225])])\n",
        "        self.transform_test = transforms.Compose([#transforms.Resize((224, 224)),   # Use this when training with original images\n",
        "                                             transforms.ToTensor(),\n",
        "                                             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                  std=[0.229, 0.224, 0.225])])\n",
        "        self.trainset = SIIMDataset(self.df_train[self.df_train['patient_id'].isin(pid_train)], self.transform_train)\n",
        "        self.valset = SIIMDataset(self.df_train[self.df_train['patient_id'].isin(pid_val)], self.transform_test)\n",
        "        self.testset = SIIMDataset(df_test, self.transform_test, test=True)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # What to do with a batch in a forward. Usually simple if everything is already defined in the model.\n",
        "        return self.model(batch['image'])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # This is called at the start of training\n",
        "        pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # Simply define a pytorch dataloader here that will take care of batching. Note it works well with dictionnaries !\n",
        "        train_dl = tdata.DataLoader(self.trainset, batch_size=self.hparams.batch_size, shuffle=True,\n",
        "                                    num_workers=os.cpu_count())\n",
        "        return train_dl\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # Same but for validation. Pytorch lightning allows multiple validation dataloaders hence why I return a list.\n",
        "        val_dl = tdata.DataLoader(self.valset, batch_size=self.hparams.batch_size, shuffle=False,\n",
        "                                  num_workers=os.cpu_count()) \n",
        "        return [val_dl]\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        test_dl = tdata.DataLoader(self.testset, batch_size=self.hparams.batch_size, shuffle=False,\n",
        "                                  num_workers=os.cpu_count()) \n",
        "        return [test_dl]\n",
        "    \n",
        "\n",
        "    def loss_function(self, logits, gt):\n",
        "        # How to calculate the loss. Note this method is actually not a part of pytorch lightning ! It's only good practice\n",
        "        #loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([32542/584]).to(logits.device))  # Let's rebalance the weights for each class here.\n",
        "        loss_fn = FocalLoss(logits=True)\n",
        "        gt = gt.float()\n",
        "        loss = loss_fn(logits, gt)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Optimizers and schedulers. Note that each are in lists of equal length to allow multiple optimizers (for GAN for example)\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams.lr, weight_decay=3e-6)\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=10 * self.hparams.lr, \n",
        "                                                        epochs=self.hparams.epochs, steps_per_epoch=len(self.train_dataloader()))\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # This is where you must define what happens during a training step (per batch)\n",
        "        logits = self(batch)\n",
        "        loss = self.loss_function(logits, batch['target']).unsqueeze(0)  # You need to unsqueeze in case you do multi-gpu training\n",
        "        # Pytorch lightning will call .backward on what is called 'loss' in output\n",
        "        # 'log' is reserved for tensorboard and will log everything define in the dictionary\n",
        "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # This is where you must define what happens during a validation step (per batch)\n",
        "        logits = self(batch)\n",
        "        loss = self.loss_function(logits, batch['target']).unsqueeze(0)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return {'val_loss': loss, 'probs': probs, 'gt': batch['target']}\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        logits = self(batch)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return {'probs': probs}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        # This is what happens at the end of validation epoch. Usually gathering all predictions\n",
        "        # outputs is a list of dictionary from each step.\n",
        "        avg_loss = torch.cat([out['val_loss'] for out in outputs], dim=0).mean()\n",
        "        probs = torch.cat([out['probs'] for out in outputs], dim=0)\n",
        "        gt = torch.cat([out['gt'] for out in outputs], dim=0)\n",
        "        probs = probs.detach().cpu().numpy()\n",
        "        gt = gt.detach().cpu().numpy()\n",
        "\n",
        "        auc_roc = torch.tensor(roc_auc_score(gt, probs))\n",
        "        tensorboard_logs = {'val_loss': avg_loss, 'auc': auc_roc}\n",
        "        print(f'Epoch {self.current_epoch}: {avg_loss:.2f}, auc: {auc_roc:.4f}')\n",
        "\n",
        "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "    \n",
        "    def test_epoch_end(self, outputs):\n",
        "        probs = torch.cat([out['probs'] for out in outputs], dim=0)\n",
        "        probs = probs.detach().cpu().numpy()\n",
        "        self.test_predicts = probs  # Save prediction internally for easy access\n",
        "        # We need to return something \n",
        "        return {'dummy_item': 0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKrwh6uPZzrC"
      },
      "source": [
        "\n",
        "## Training\n",
        "\n",
        "Let's start by specifying parameters, the seed and output folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ruW9GOz-ZzrC"
      },
      "source": [
        "# dict_to_args is a simple helper to make hparams act like args from argparse. This makes it trivial to then use argparse\n",
        "OUTPUT_DIR = './lightning_logs'\n",
        "hparams = dict_to_args({'batch_size': 64,\n",
        "                        'lr': 1e-4, # common when using pretrained\n",
        "                        'epochs': 10,\n",
        "                        'arch': 'seresnext50'\n",
        "                       })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VjCe80EZzrC"
      },
      "source": [
        "For training we just need to instantiate the pytorch lightning module and a trainer with a few options. Most importantly this is where you specify how many GPU to use (or TPU) and if you want to do mixed precision training (with apex). For the purpose of this kernel I just do FP32 1GPU training but please read the pytorch lightning doc if you want to try TPU and/or mixed precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "T7r-rCMFZzrC",
        "outputId": "eb3f1b0d-2ae6-4569-aa2c-6f4eaf10082a"
      },
      "source": [
        "# Initiate model\n",
        "model = LightModel(train_df, test_df, pid_train, pid_val, hparams)\n",
        "tb_logger = pl.loggers.TensorBoardLogger(save_dir='./',\n",
        "                                         name=f'baseline', # This will create different subfolders for your models\n",
        "                                         version=f'0')  # If you use KFold you can specify here the fold number like f'fold_{fold+1}'\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(filepath=tb_logger.log_dir + \"/{epoch:02d}-{auc:.4f}\",\n",
        "                                                   monitor='auc', mode='max')\n",
        "# Define trainer\n",
        "# Here you can \n",
        "trainer = pl.Trainer(max_nb_epochs=hparams.epochs, auto_lr_find=False,  # Usually the auto is pretty bad. You should instead plot and pick manually.\n",
        "                     gradient_clip_val=1,\n",
        "                     nb_sanity_val_steps=0,  # Comment that out to reactivate sanity but the ROC will fail if the sample has only class 0\n",
        "                     checkpoint_callback=checkpoint_callback,\n",
        "                     gpus=1,\n",
        "                     early_stop_callback=False,\n",
        "                     progress_bar_refresh_rate=0\n",
        "                     )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_semi-supervised-ImageNet1K-models_master\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-92f84331f2b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLightModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m tb_logger = pl.loggers.TensorBoardLogger(save_dir='./',\n\u001b[1;32m      4\u001b[0m                                          \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'baseline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# This will create different subfolders for your models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          version=f'0')  # If you use KFold you can specify here the fold number like f'fold_{fold+1}'\n",
            "\u001b[0;32m<ipython-input-32-b9df23ed2698>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df_train, df_test, pid_train, pid_val, hparams)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# You will obviously want to make the model better :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Defining datasets here instead of in prepare_data usually solves a lot of problems for me...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mbuffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2njwEOwcZzrD"
      },
      "source": [
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP3Ipn7yZzrD"
      },
      "source": [
        "## Test time\n",
        "The easy part :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "f9Nu4bdUZzrD"
      },
      "source": [
        "# Grab best checkpoint file\n",
        "out = Path(tb_logger.log_dir)\n",
        "aucs = [ckpt.stem[-4:] for ckpt in out.iterdir()]\n",
        "best_auc_idx = aucs.index(max(aucs))\n",
        "best_ckpt = list(out.iterdir())[best_auc_idx]\n",
        "print('Using ', best_ckpt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "g2i1i9GLZzrD"
      },
      "source": [
        "trainer = pl.Trainer(resume_from_checkpoint=str(best_ckpt), gpus=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mjyvBM_3ZzrD"
      },
      "source": [
        "trainer.test(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0qrXROjTZzrE"
      },
      "source": [
        "preds = model.test_predicts\n",
        "test_df['target'] = preds\n",
        "submission = test_df[['image_name', 'target']]\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yLNlajEbZzrE"
      },
      "source": [
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xUIHva2iZzrE"
      },
      "source": [
        "submission.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ewQP-yGZzrE"
      },
      "source": [
        "## Suggested improvements\n",
        "\n",
        "* KFold validation + Average predictions on test\n",
        "* Explore architectures (efficient\n",
        "* Heavy data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eS6oaOIAZzrF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}